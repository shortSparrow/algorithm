from typing import List

import numpy as np
from knn.calculate_distance import calculate_distance
from knn.find_k_nearest_neighbors import find_k_nearest_neighbors
from knn.predict_label import predict_label

# Опис того, як працює алгоритм
# Запускаємо цикл по X_test
# Визначаємо відстань від поточного елемента у X_test до усіх елементів у X_train. Варто пам'ятати що жодного
# елемента з X_test немає у X_train. Відстань визначаємо за номою L2 (Евклідова норма)
#
# Маючи відстані від поточного елемента X_test до всіх всіх елементів з X_train запускаємо find_k_nearest_neighbors.
# У find_k_nearest_neighbors ми передаємо відстані які знайшли і y_train (labels - тобто правильні цифри які відповідають
# картинкам з X_train), і там об'єднуємо відстані і labels. Пам'ятаємо що порядок у них однаковий, бо відстані - це
# відстані до елементів X_train, тобто вони записані у порядку у якому і елементи X_train, і так само labels - бо це
# labels саме елементів X_train, тож вони записані також у порядку у якому йде X_train. 
# І об'єднавши відстані і labels повертаємо k найближчих елементів до поточного елементу X_test. Тож 
# find_k_nearest_neighbors повертає tuple з відстані і label (правильного числа)
#
# Маючи k найближчих елементів просто дивимось скільки з них відповідають якому числу (label) - це і буде 
# відповідь для нашого елемента X_test, до якої цифри він відноситься
#
# Далі додаємо результат у predictions і повторюємо знову для наступного елемента X_test



# X_train: List[List[int]]
#   Призначення: Це навчальний набір даних.Кожен елемент у цьому списку є окремим навчальним зразком
#                (у нашому випадку, зображенням рукописної цифри). Цифри будуть від 0 до 16 і це 
#                інтенсивність сірого, від 0 - чорного до 16 - білого
#   Приклад: [[0, 0, 5, 13,...], [1, 2, 3, 4,...], ...]
#
# y_train: List[int]
#   Призначення: Це навчальний набір міток (labels), що відповідають кожному зображенню в X_train.
#                Це "правильні відповіді" для кожного навчального зображення. Тобто це буквально написані
#                цифрами ті цифри, що зображені на малюнках в X_train
#   Приклад: [5, 0, 4, 1, 9, 2, ...]
# 
# X_test: List[List[int]]
#   Призначення: Це тестовий набір даних ознак. Ці зображення модель ніколи не "бачила" під час навчання,
#                і ми хочемо, щоб вона їх класифікувала.  Цифри будуть від 0 до 16 і це 
#                інтенсивність сірого, від 0 - чорного до 16 - білого
#   Приклад:  [[0, 0, 5, 13,...], [1, 2, 3, 4,...], ...]
#
# k: int - визначає скільки найближчих сусідів треба брати

def my_knn_vectorization(
    X_train: List[List[int]], 
    y_train: List[int], 
    X_test: List[List[int]], 
    k: int
) -> List[int]:
    # Конвертуємо вхідні дані до NumPy масивів для ефективних обчислень
    _X_train = np.array(X_train)
    _y_train = np.array(y_train)
    _X_test = np.array(X_test)
    
    # Цей список буде зберігати прогнозовані мітки для кожного тестового зображення
    predictions = []

    for test_image in _X_test:
        # test_image і _X_train мають однакові розмірність - це масиви чисел 
        # test_image (одне зображення) віднімається від кожного зображення в _X_train (усіх навчальних зображень).
        # NumPy, використовуючи broadcasting, автоматично "розтягує" test_image для виконання поелементного віднімання.
        # Результатом є масив, де кожен рядок містить різниці пікселів між test_image і відповідним навчальним зображенням
        # 
        # squared_differences має форму [ [0., 88., 12., 33.,...],... ] (N, 64), де N — кількість навчальних зображень
        squared_differences = (test_image - _X_train)**2
        
        # Сумуємо різницю квадратів 
        # Параметр axis=1 вказує NumPy, що сумувати потрібно вздовж осі, яка відповідає пікселям
        # (тобто, сумувати всі 64 значення для кожного рядка/зображення). 
        # Форма sum_squared_differences - [1605., 2352., 3557., 978., ....] (N, 1), де N — кількість навчальних зображень
        # Форма distances - [40.06, 48.49, 69.64, 31.27., ....] (N, 1), де N — кількість навчальних зображень
        sum_squared_differences = np.sum(squared_differences, axis=1)
        distances = np.sqrt(sum_squared_differences)


        # np.stack об'єднає у масив distances і y_train
        # combined - [ [distances[0], y_train[0]], [distances[1], y_train[1]], ...]
        # *Елементи distances мають тип float, а елементи _y_train тип int і особливість numpy в тому що, 
        # *при об'єднанні numpy підніме тип int до типу float, і combined буде масивом масивів float 
        combined = np.stack((distances, _y_train), axis=1)
        
        # Отримуємо індекси, які сортують ПЕРШИЙ стовпець (відстані)
        # combined[:, 0] дістає перші елементи з підмасивів combined, тобто елементи distances 
        # sorted_indices - це індекси якби масив був відсортований. np.argsort не сортує масив, він повертає 
        # індекси у порядку якби масив був би відсортований
        sorted_indices = np.argsort(combined[:, 0])

        # Застосовуємо індекси до всього масиву
        sorted_neighbors = combined[sorted_indices] 
        
        # Дістаємо найближчі k елементів до нашого зображення
        k_nearest_neighbors = sorted_neighbors[:k]

        # важливо розуміти що predict_label очікує, що k_nearest_neighbors буде [ [float, int],... ], 
        # а у нас це [ [float, float],... ]. Оскільки функція predict_label писалась для рішення без векторизації 
        # і numpy то я не хочу її змінювати. Але тут треба бути уважними
        predicted_label = predict_label(k_nearest_neighbors)
        # перетворюємо в int щоб label - себто самці цифри були int, а не float, бо на картинках ж вони int
        predictions.append(int(predicted_label))

    return predictions
