import numpy as np
from k_means.initialize_centroids import initialize_centroids

"""
 Це копія функції run_kmeans, але тут прохід по циклу data, що був найбільш
 повільним місцем у run_kmeans замінений на векторизацію, і це значно пришвидшує код 
"""

def run_kmeans_with_vectorization(data: np.ndarray, k: int, max_iter=100):
    centroids = initialize_centroids(data=data, k=k)
    print(f"initial centroids: {centroids}")
    
    for iter_num in range(max_iter):
        # --- Крок 1: отримати масив labels_array який вказує який піксель до якого центроїда найближчий ---
        # --- Присвоєння точок до кластерів (ВЕКТОРИЗОВАНО) ---

        # Розширюємо розмірності для broadcasting
        # data було [[r,g,b],[r,g,b],[r,g,b],[r,g,b]...]
        # data_expanded стане [ [[r,g,b]], [[r,g,b]], [[r,g,b]], [[r,g,b]], ... ]
        # перші : - це вибір всіх елементів по першій осі (пікселів)
        # np.newaxis створить нову вісь (розмірність) там де раніше були [r,g,b]
        # другі : - вибере всі елементи по третій осі, яка раніше була другою, для нас це елементи [r,g,b]
        # 
        # Якщо підсумувати, то до цієї операції data мала форму (N, 3), а після операції data_expanded матиме форму (N, 1, 3)
        data_expanded = data[:, np.newaxis, :]  # (num_pixels, 1, 3)

        # centroids - [[r,g,b], [r,g,b], [r,g,b], [r,g,b], ...] Довжина centroids = 12
        # centroids[np.newaxis, :, :] - [[ [r,g,b], [r,g,b], [r,g,b], [r,g,b], ... ]]
        # np.newaxis - створює нову вісь
        # перші : - копіюють вісь яка раніше була віссю 0, а тепер буде віссю 1
        # другі : - копіюють вісь яка раніше була віссю 1, а тепер буде віссю 2 (тобто вибираємо самі [r,g,b])
        # centroids_expanded тепер має форму (1, k, 3)
        centroids_expanded = centroids[np.newaxis, :, :]


        # НАВІЩО БУЛО ЗМІНЮВАТИ ФОРМУ data і centroids
        #
        # Ми змінювали форму data і centroids для того щоб зробити Broadcasting (трансляції) в NumPy,
        # щоб ми могли ефективно (і дуже швидко) обчислити відстані від кожного пікселя до кожного центроїда одночасно
        #
        # data: Містить усі пікселі зображення. Кожен піксель має 3 колірні компоненти (R, G, B).
        # Форма: (N, 3), де N — загальна кількість пікселів.
        #   Приклад: [[R0, G0, B0], [R1, G1, B1], ..., [RN-1, GN-1, BN-1]]
        #
        # centroids: Містить поточні координати k центроїдів. Кожен центроїд також має 3 колірні компоненти.
        # Форма: (k, 3), де k — бажана кількість кластерів.
        #   Приклад: [[C0_R, C0_G, C0_B], [C1_R, C1_G, C1_B], ..., [CK-1_R, CK-1_G, CK-1_B]]
        #
        #
        # Нам потрібно обчислити Евклідову відстань між кожним з N пікселів та кожним з k центроїдів.
        # Це призведе до матриці відстаней форми (N, k)
        # Пряме віднімання data - centroids не працюватиме, оскільки їхні форми (N, 3) і (k, 3) несумісні
        # для поелементного віднімання. Ми не можемо відняти матрицю 1x3 від матриці 2x3
        #
        # Але після змін ми матимемо матриці (N, 1, 3) і (1, k, 3)
        #
        # data_expanded - (273280,1,3)
        # centroids_expanded - (1, 12, 3)
        # Коли виконується операція віднімання (-), NumPy перевіряє розмірності масивів, починаючи з кінця:
        #   1. Остання вісь (розмірність кольору): 3 і 3. Вони збігаються, все добре. 
        #   2. Середня вісь: 1 (з data_expanded) і k (з centroids_expanded). NumPy бачить 1 і k.
        #      Правила Broadcasting говорять, що якщо одна з розмірностей дорівнює 1, вона може бути "розтягнута"
        #      (broadcasted) до розміру іншої. Отже, 1 "розтягується" до k. 
        #   3. Перша вісь: N (з data_expanded) і 1 (з centroids_expanded). Аналогічно, 1 "розтягується" до N.
        # 
        # Що таке розтягування
        # Не плутати з розтягуванням (scaling) вектора в лінійній алгебрі (коли множимо вектор на скаляр), це ІНШЕ.
        # Коли NumPy "розтягує" вісь, він логічно уявляє, що елементи вздовж цієї осі повторюються, щоб "заповнити"
        # більший розмір. Фізично копії не створюються у пам'яті (за рідкісними винятками). NumPy просто змінює спосіб,
        # яким він звертається до даних.
        # Тож грубо кажучи після розтягування ми маємо однакові форми: data_expanded (N, k, 3) і centroids_expanded (N, k, 3)
        # І оскільки тепер форми однакові то можна виконувати операції додавання/віднімання/множення і т.п.


        # Обчислюємо різниці, підносимо до квадрату, сумуємо по компонентах (RGB)
        # Все це відбувається ОДНИМ ОПЕРАЦІЯМ NumPy
        # (point1 - point2)^2 
        # 
        # squared_differences виглядає так - [ [[r,g,b],[r,g,b],[r,g,b], ...], [[r,g,b],[r,g,b],[r,g,b], ...], ... ]
        # Довжина squared_differences - 273280 елементів 
        # кожен елемент - це масив з 12 піделементів, кожен з яких це [r,g,b]
        # тобто кожен піксель (елемент з 273280) містить у собі відстань до всіх центроїдів 
        # 
        # data_expanded - [ [[139,139,129]], ...]
        # centroids_expanded - [[ [127,138,80],... ]]
        # squared_differences - [ [[144,1,97], ...],... ]
        # (139-127)^2 = 144
        # (139-138)^2 = 1
        # (129-138)^2 = 2401 // 256 => 97 (Оскільки наші значення можуть бути у діапазоні від 0 до 256) 
        #               то беремо залишок від ділення і все сходиться
        # squared_differences = [ [[144,1,97],...],... ]
        squared_differences = (data_expanded - centroids_expanded)**2 
        
        # np.sum по останній осі (RGB) 
        # Пам'ятаймо що ми рахуємо відстань від векторів (пікселів) дл цетроїда за допомогою теореми Піфагора 
        # math.sqrt( (r1 - r2)^2 + (g1 - g2)^2 + (b1 - b2)^2 ) 
        # 
        # У squared_differences ми маємо віднімання і піднесення до квадрату кожної точки з кожним центроїдом
        # Тепер нам треба додати r,g,b кожної точки - 144 + 1 + 97 = 242
        # sum_squared_differences - [ [242, ...],... ] 
        # axis - це вісь по якій треба робить операцію, вона починається з 0 і у нас максимум може бути 2
        sum_squared_differences = np.sum(squared_differences, axis=2)

       
        # Беремо квадратний корінь для отримання Евклідових відстаней 
        # 
        # Фінальний етап розрахунку відстані, маючи суму всіх кольорів кожного пікселя до кожного центроїда, заносимо все під корінь
        # sqrt(242) = 15.55...
        # distances - [ [15.55, ...],... ]
        # distances - це масив з довжиною 273280, де кожен елемент - це відстань конкретно цього пікселя до всіх k центроїдів 
        distances = np.sqrt(sum_squared_differences) # Форма (num_pixels, k)

        # Знаходимо індекс найближчого центроїда для кожного пікселя
        # distances = [
        #     [d(p0, c0), d(p0, c1), d(p0, c2),...],  # Відстані пікселя 0 до центроїдів 0, 1, 2, ...
        #     [d(p1, c0), d(p1, c1), d(p1, c2),...],  # Відстані пікселя 1 до центроїдів 0, 1, 2, ...
        #     [d(p2, c0), d(p2, c1), d(p2, c2),...],  # Відстані пікселя 2 до центроїдів 0, 1, 2, ...
        #     [d(p3, c0), d(p3, c1), d(p3, c2),...]   # Відстані пікселя 3 до центроїдів 0, 1, 2, ...
        #      ...
        # ] 
        # 
        # axis=1: Це ключовий параметр, який вказує, вздовж якої осі потрібно шукати мінімум.
        #
        # Візьмемо довільний приклад
        # distances = [
        #     [5.0, 1.0, 8.0],  # Піксель 0: відстані до C0, C1, C2
        #     [3.0, 7.0, 2.0],  # Піксель 1: відстані до C0, C1, C2
        #     [9.0, 4.0, 6.0],  # Піксель 2: відстані до C0, C1, C2
        #     [1.5, 2.5, 0.5]   # Піксель 3: відстані до C0, C1, C2
        # ]
        # Для першого рядка (Піксель 0): Мінімальне значення: 1.0, його індекс 1, тож він належить до центроїда з індексом 1
        # Для другого рядка (Піксель 1): Мінімальне значення: 2.0,  його індекс 2, тож він належить до центроїда з індексом 2
        # і так далі
        # Тож на виході маємо labels_array = [1, 2, 1, 2]
        # labels_array - це масив довжиною 273280. Це масив який вказує який піксель до якого центроїда найближчий
        labels_array = np.argmin(distances, axis=1)



        # --- Крок 2: Оновлення центроїдів (можемо залишити твій код, але його також можна векторизувати) ---

        # Зберігаємо старі центроїди для перевірки збіжності
        old_centroids = centroids.copy()

        # Створюємо пустий масив для нових центроїдів, того ж розміру, що й старі
        # new_centroids - [ [0., 0., 0.], [0., 0., 0.]... ] має довжину k - для нас це 12
        new_centroids = np.zeros_like(old_centroids, dtype=float) 

        for cluster_idx in range(k):
            # 1. Створюємо булеву маску: знаходимо всі пікселі, які належать до поточного кластера 'cluster_idx'
            # labels_array == cluster_idx поверне масив типу [True, False, False, True, ...]
            # де True означає, що піксель належить до кластера 'cluster_idx'
            points_in_cluster_mask = (labels_array == cluster_idx) 

            # 2. Вибираємо пікселі за маскою:
            # data[points_in_cluster_mask] поверне новий NumPy масив, що містить ТІЛЬКИ пікселі,
            # які належать до кластера 'cluster_idx'.
            # Його форма буде (кількість_пікселів_у_кластері, 3) 
            # points_in_this_cluster - [ [r,g,b], [r,g,b], [r,g,b], ... ]
            points_in_this_cluster = data[points_in_cluster_mask]

            # 3. Обчислюємо середнє значення для цих пікселів:
            # .shape[0] показує скільки елементів у масиві
            # .shape[1] скільки піделементів у одному елементів, для нас це 3 (r,g,b)
            if points_in_this_cluster.shape[0] > 0: # Перевіряємо, чи є пікселі в цьому кластері
                # .mean(axis=0) обчислює середнє для КОЖНОГО стовпця окремо.
                # points_in_this_cluster = [
                #     [R1, G1, B1] 
                #     ...
                #     [R5, G5, B5]
                #     ...
                #     [R10, G10, B10]
                # ] 
                # 
                # mean(axis=0) рахує так:
                # Середнє R: (R1 + R5 + R10) / 3
                # Середнє G: (G1 + G5 + G10) / 3
                # Середнє B: (B1 + B5 + B10) / 3
                # 
                # mean(axis=1) рахує середнє для кожного пікселя
                # Для [R1, G1, B1], середнє буде (R1 + G1 + B1) / 3 
                # Для [R5, G5, B5], середнє буде (R5 + G5 + B5) / 3.
                # 
                # Саме тому використовуємо mean(axis=0)
                # points_in_this_cluster.mean(axis=0) - поверне один масив на 3 елементи [r,g,b]
                new_centroids[cluster_idx] = points_in_this_cluster.mean(axis=0)
            else:
                # Якщо кластер порожній, залишаємо його центроїд незмінним
                # Теоретично можна зробити якусь додаткову обробку, бо тоді виходить що ми хибно обрали центроїд, 
                # він надто далеко від усіх, але для тут мені це не потрібно
                new_centroids[cluster_idx] = old_centroids[cluster_idx]    


        # --- Крок 3: Перевірка на збіжність ---
        # Порівнюємо нові центроїди зі СТАРИМИ центроїдами за допомогою np.allclose
        # rtol і atol
        # Для того щоб .allclose повернув True треба щоб виконувалася умова ∣a−b∣ ≤ atol+rtol×∣b∣ 
        # ∣a−b∣ - це абсолютна різниця між елементами
        # 
        # rtol (relative tolerance): відносна толерантність. Це поріг для відносної різниці між двома числами.
        # Вона стає важливою, коли порівнюються великі числа. Чим більші числа, тим більшою може бути абсолютна
        # різниця між ними, щоб вони все ще вважалися "майже рівними". 
        # Приклад: 
        #   Якщо rtol=1e-5 (0.00001), то два числа 100000.00 та 100000.01 будуть вважатися "майже рівними",
        #   тому що їхня відносна різниця мала: лише 0.01 
        #   Далі число b (100000.01) множиться на  rtol=1e-5 -> 100000.01 * 1e-5 = 1.0000001 
        #   І оскільки 0.01 ≤ 1.0000001 то тоді .allclose поверне true
        # Іншими словами можна сказати, що rtol=1e-5 (0.00001) це допустима відмінність у 0.001% між двома числами
        # 
        # 
        # atol (absolute tolerance): абсолютна толерантність. Це поріг для абсолютної різниці між двома числами.
        # Вона стає критично важливою, коли порівнювані числа дуже малі, або одне з них є нулем.
        # Приклад: 
        #   Якщо atol=1e-8 (0.00000001), то числа 1e-9 та 2e-9 будуть вважатися "майже рівними", навіть якщо їх відносна різниця велика
        is_converged = np.allclose(new_centroids, old_centroids, rtol=1e-5, atol=1e-8) 
        
        # Оновлюємо центроїди для наступної ітерації
        centroids = new_centroids 

        if is_converged:
            print(f"K-Means зійшовся на ітерації {iter_num + 1}.")
            break
       
    else:
        print(f"K-Means досяг максимальної кількості ітерацій ({max_iter}).")
    
    return centroids.astype(int), labels_array